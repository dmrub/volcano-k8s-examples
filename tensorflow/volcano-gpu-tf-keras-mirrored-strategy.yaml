# Volcano example for tensorflow MirroredStrategy
# We use one replic with multiple GPUs
# Based on
# https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb
# https://github.com/volcano-sh/volcano/blob/master/docs/user-guide/how_to_use_svc_plugin.md
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: tf-keras-mirrored-strategy
  namespace: dmrub
spec:
  minAvailable: 1 # minimal number of pods
  schedulerName: volcano
  queue: default
  plugins:
    env: []
    svc: []
  policies:
    - event: PodEvicted
      action: RestartJob
  tasks:
    - replicas: 1 # number of pods
      name: worker
      policies:
        - event: TaskCompleted
          action: CompleteJob
      template:
        spec:
          imagePullSecrets:
            - name: default-secret
          containers:
            - name: tensorflow
              image: tensorflow/tensorflow:2.9.1-gpu
              # image: ghcr.io/dfki-asr/tensorflow:2.9.1-gpu-fix
              imagePullPolicy: Always
              env:
                - name: MODEL_PATH
                  value: ./keras-model # /tmp/keras-model
                - name: TEMP_MODEL_PATH
                  value: ./keras-model-temp # /tmp/keras-model # /var/data/keras-model
              command:
                - /bin/bash
                - -c
                - |
                  set -xe;
                  nvidia-smi;
                  nvidia-smi --format=csv,noheader --query-gpu=name,serial,uuid,pci.bus_id;
                  ulimit -c unlimited;
                  python3 main_mirrored_strategy.py;
                  ERR=$?;
                  echo "Error: $ERR";
                  if [ $ERR -eq 134 ]; then
                    ls -lah;
                    echo "Process PID: $$";
                    while true; do sleep 10; done;
                  fi
              ports:
                - containerPort: 2222
                  name: tfjob-port
              resources:
                limits:
                  nvidia.com/gpu: "3" # <----- number of GPUs per pod
              workingDir: /data/volcano-k8s-examples/tensorflow/src
              volumeMounts:
                - mountPath: /data
                  name: data-volume
          volumes:
            - name: data-volume
              persistentVolumeClaim:
                claimName: volcano-workspace
          restartPolicy: OnFailure
